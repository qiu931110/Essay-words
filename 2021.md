
1：an alternative framework to self-attention ： 用于替换self-attention的框架
alternative：选择性的

2 lambda layers bypass expensive attention maps：lambda层绕过了昂贵的注意力地图
bypass：绕过

3 transforming available contexts into linear functions：将可用的上下文语境转换成线性函数
transforming into ：转换成
contexts：上下文语境

4 significantly outperform their convolutional and attentional counterparts on ImageNet classification：在ImageNet分类任务上明显胜过其卷积和注意力的对应操作
significantly outperform：明显胜过
counterparts：同行

